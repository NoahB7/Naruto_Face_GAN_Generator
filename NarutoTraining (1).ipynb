{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NarutoTraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "dk1MLArEqFf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSy3YaP49yPO",
        "outputId": "4bc6ba28-cb68-416b-8ce4-e84129dc928f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "XqCifAOuCn5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "f = 'archive.zip'\n",
        "\n",
        "with ZipFile(f, 'r') as z:\n",
        "  z.extractall()\n",
        "  print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CN7yBgKq5_U",
        "outputId": "c5df5891-e43b-442b-9346-53db9b5a515d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"naruto-face-dataset/images-big\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhaQsox6qJQW",
        "outputId": "e4f744fd-5fd8-483a-a4de-863e3dd7f7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3838 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "BeChDjooqKfV",
        "outputId": "90323cfc-f81f-4cbe-8195-fc96863dd28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkyI8MwGqMFi",
        "outputId": "37856051-1026-4d57-873d-6018a3123284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404,801\n",
            "Trainable params: 404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDfSkBIsqNKd",
        "outputId": "1cfec92e-7b21-4e55-e0ee-3eb78ee5b74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              1056768   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      262272    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 3)         38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,979,651\n",
            "Trainable params: 3,979,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "5Eek1l9wqPKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save(\"naruto-faces-long/generated_img_%03d_%d.png\" % (epoch, i))"
      ],
      "metadata": {
        "id": "phPJA6_GqQmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 800  # In practice, use ~100 epochs\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_yPEyRNqRys",
        "outputId": "47d4e41b-000a-4584-8c78-4d3bda8c542c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "120/120 [==============================] - 98s 707ms/step - d_loss: 0.3951 - g_loss: 1.6547\n",
            "Epoch 2/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.1060 - g_loss: 4.5907\n",
            "Epoch 3/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: -1.1956 - g_loss: 124.0831\n",
            "Epoch 4/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 1.6719 - g_loss: 4.0144\n",
            "Epoch 5/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: 0.7960 - g_loss: 2.0247\n",
            "Epoch 6/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7616 - g_loss: 1.6698\n",
            "Epoch 7/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7516 - g_loss: 1.0889\n",
            "Epoch 8/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7087 - g_loss: 1.2131\n",
            "Epoch 9/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6170 - g_loss: 1.0850\n",
            "Epoch 10/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5119 - g_loss: 1.4105\n",
            "Epoch 11/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6927 - g_loss: 1.0730\n",
            "Epoch 12/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.4116 - g_loss: 1.5673\n",
            "Epoch 13/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5123 - g_loss: 1.1535\n",
            "Epoch 14/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5952 - g_loss: 1.1986\n",
            "Epoch 15/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: 0.9074 - g_loss: 1.3265\n",
            "Epoch 16/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: 0.7352 - g_loss: 1.4413\n",
            "Epoch 17/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: 0.4925 - g_loss: 1.2730\n",
            "Epoch 18/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5719 - g_loss: 1.3241\n",
            "Epoch 19/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5505 - g_loss: 1.3650\n",
            "Epoch 20/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6471 - g_loss: 0.9778\n",
            "Epoch 21/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5988 - g_loss: 1.0613\n",
            "Epoch 22/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6001 - g_loss: 0.9697\n",
            "Epoch 23/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7127 - g_loss: 1.7504\n",
            "Epoch 24/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7033 - g_loss: 1.3583\n",
            "Epoch 25/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6741 - g_loss: 1.2652\n",
            "Epoch 26/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.8045 - g_loss: 1.0397\n",
            "Epoch 27/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7910 - g_loss: 1.1911\n",
            "Epoch 28/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7624 - g_loss: 1.4576\n",
            "Epoch 29/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6427 - g_loss: 1.0045\n",
            "Epoch 30/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6159 - g_loss: 1.2429\n",
            "Epoch 31/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.7322 - g_loss: 1.1630\n",
            "Epoch 32/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.7006 - g_loss: 1.2845\n",
            "Epoch 33/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.6677 - g_loss: 0.9477\n",
            "Epoch 34/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6869 - g_loss: 1.0568\n",
            "Epoch 35/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.7233 - g_loss: 1.1656\n",
            "Epoch 36/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.7223 - g_loss: 1.1905\n",
            "Epoch 37/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4017 - g_loss: 2.5579\n",
            "Epoch 38/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6572 - g_loss: 1.3151\n",
            "Epoch 39/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6047 - g_loss: 1.1822\n",
            "Epoch 40/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.5388 - g_loss: 1.1859\n",
            "Epoch 41/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.5636 - g_loss: 1.0662\n",
            "Epoch 42/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.5852 - g_loss: 1.1007\n",
            "Epoch 43/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6013 - g_loss: 1.3633\n",
            "Epoch 44/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4915 - g_loss: 2.0778\n",
            "Epoch 45/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: 0.6499 - g_loss: 1.1735\n",
            "Epoch 46/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6709 - g_loss: 1.2659\n",
            "Epoch 47/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.6512 - g_loss: 1.0161\n",
            "Epoch 48/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6657 - g_loss: 1.3037\n",
            "Epoch 49/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.5939 - g_loss: 1.1628\n",
            "Epoch 50/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.5818 - g_loss: 0.9796\n",
            "Epoch 51/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6417 - g_loss: 1.3377\n",
            "Epoch 52/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6386 - g_loss: 1.0017\n",
            "Epoch 53/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6119 - g_loss: 1.1822\n",
            "Epoch 54/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5508 - g_loss: 1.3705\n",
            "Epoch 55/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5939 - g_loss: 1.0578\n",
            "Epoch 56/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7862 - g_loss: 1.1047\n",
            "Epoch 57/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.5896 - g_loss: 1.3644\n",
            "Epoch 58/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7265 - g_loss: 1.5719\n",
            "Epoch 59/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5130 - g_loss: 1.7217\n",
            "Epoch 60/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6463 - g_loss: 1.3558\n",
            "Epoch 61/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4581 - g_loss: 1.4142\n",
            "Epoch 62/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.5568 - g_loss: 1.2877\n",
            "Epoch 63/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.5557 - g_loss: 1.1498\n",
            "Epoch 64/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6336 - g_loss: 1.0688\n",
            "Epoch 65/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5854 - g_loss: 1.4474\n",
            "Epoch 66/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4757 - g_loss: 1.4773\n",
            "Epoch 67/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6350 - g_loss: 1.2345\n",
            "Epoch 68/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6095 - g_loss: 1.2068\n",
            "Epoch 69/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6185 - g_loss: 1.4976\n",
            "Epoch 70/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6067 - g_loss: 1.1162\n",
            "Epoch 71/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6363 - g_loss: 1.1922\n",
            "Epoch 72/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6122 - g_loss: 1.4220\n",
            "Epoch 73/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.7589 - g_loss: 1.0852\n",
            "Epoch 74/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6394 - g_loss: 1.1008\n",
            "Epoch 75/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6421 - g_loss: 0.9631\n",
            "Epoch 76/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6495 - g_loss: 0.8945\n",
            "Epoch 77/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5991 - g_loss: 1.1695\n",
            "Epoch 78/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6571 - g_loss: 0.9256\n",
            "Epoch 79/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6350 - g_loss: 0.9378\n",
            "Epoch 80/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6740 - g_loss: 1.0630\n",
            "Epoch 81/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.8082 - g_loss: 1.7924\n",
            "Epoch 82/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5035 - g_loss: 1.7265\n",
            "Epoch 83/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7285 - g_loss: 1.0900\n",
            "Epoch 84/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6468 - g_loss: 1.0080\n",
            "Epoch 85/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5681 - g_loss: 1.1008\n",
            "Epoch 86/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7584 - g_loss: 1.2191\n",
            "Epoch 87/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6898 - g_loss: 1.0203\n",
            "Epoch 88/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6094 - g_loss: 1.2635\n",
            "Epoch 89/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6380 - g_loss: 1.0329\n",
            "Epoch 90/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7044 - g_loss: 0.9458\n",
            "Epoch 91/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5388 - g_loss: 1.4004\n",
            "Epoch 92/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.5741 - g_loss: 1.0478\n",
            "Epoch 93/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.7104 - g_loss: 0.9601\n",
            "Epoch 94/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5588 - g_loss: 1.2343\n",
            "Epoch 95/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6348 - g_loss: 1.0753\n",
            "Epoch 96/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7210 - g_loss: 1.1233\n",
            "Epoch 97/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6636 - g_loss: 1.0027\n",
            "Epoch 98/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6499 - g_loss: 1.0367\n",
            "Epoch 99/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6719 - g_loss: 0.9229\n",
            "Epoch 100/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6689 - g_loss: 0.9298\n",
            "Epoch 101/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6648 - g_loss: 0.8979\n",
            "Epoch 102/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6094 - g_loss: 1.0452\n",
            "Epoch 103/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6210 - g_loss: 1.6709\n",
            "Epoch 104/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.7713 - g_loss: 1.3476\n",
            "Epoch 105/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6493 - g_loss: 1.5310\n",
            "Epoch 106/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6579 - g_loss: 1.1234\n",
            "Epoch 107/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6491 - g_loss: 1.0642\n",
            "Epoch 108/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6537 - g_loss: 0.9712\n",
            "Epoch 109/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6461 - g_loss: 0.9955\n",
            "Epoch 110/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6411 - g_loss: 1.0939\n",
            "Epoch 111/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6450 - g_loss: 1.0744\n",
            "Epoch 112/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6665 - g_loss: 0.9679\n",
            "Epoch 113/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6630 - g_loss: 0.9112\n",
            "Epoch 114/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6159 - g_loss: 1.0551\n",
            "Epoch 115/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.6501 - g_loss: 0.9380\n",
            "Epoch 116/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.7082 - g_loss: 1.0281\n",
            "Epoch 117/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6122 - g_loss: 1.0509\n",
            "Epoch 118/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6559 - g_loss: 1.0151\n",
            "Epoch 119/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6148 - g_loss: 1.1657\n",
            "Epoch 120/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6083 - g_loss: 1.1487\n",
            "Epoch 121/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6522 - g_loss: 1.3116\n",
            "Epoch 122/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6796 - g_loss: 0.9386\n",
            "Epoch 123/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6388 - g_loss: 1.0078\n",
            "Epoch 124/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6238 - g_loss: 0.9917\n",
            "Epoch 125/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6601 - g_loss: 1.2694\n",
            "Epoch 126/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6974 - g_loss: 1.3525\n",
            "Epoch 127/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.6615 - g_loss: 0.9269\n",
            "Epoch 128/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6254 - g_loss: 1.0481\n",
            "Epoch 129/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6203 - g_loss: 0.9478\n",
            "Epoch 130/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5816 - g_loss: 1.0435\n",
            "Epoch 131/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6575 - g_loss: 1.1300\n",
            "Epoch 132/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5555 - g_loss: 1.0612\n",
            "Epoch 133/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5616 - g_loss: 1.1888\n",
            "Epoch 134/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5904 - g_loss: 1.1359\n",
            "Epoch 135/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7430 - g_loss: 1.2791\n",
            "Epoch 136/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6785 - g_loss: 1.3091\n",
            "Epoch 137/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5934 - g_loss: 1.1406\n",
            "Epoch 138/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6066 - g_loss: 1.0528\n",
            "Epoch 139/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6263 - g_loss: 1.0721\n",
            "Epoch 140/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6288 - g_loss: 1.0974\n",
            "Epoch 141/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6320 - g_loss: 1.1536\n",
            "Epoch 142/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.6293 - g_loss: 1.3309\n",
            "Epoch 143/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5310 - g_loss: 1.2947\n",
            "Epoch 144/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.7213 - g_loss: 1.6873\n",
            "Epoch 145/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.6030 - g_loss: 1.1562\n",
            "Epoch 146/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.5763 - g_loss: 1.1314\n",
            "Epoch 147/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.6133 - g_loss: 1.3418\n",
            "Epoch 148/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6723 - g_loss: 0.9650\n",
            "Epoch 149/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5923 - g_loss: 1.0348\n",
            "Epoch 150/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5647 - g_loss: 1.2703\n",
            "Epoch 151/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.6455 - g_loss: 1.1876\n",
            "Epoch 152/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5796 - g_loss: 1.1620\n",
            "Epoch 153/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5385 - g_loss: 1.2418\n",
            "Epoch 154/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5963 - g_loss: 1.2164\n",
            "Epoch 155/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.6105 - g_loss: 1.4220\n",
            "Epoch 156/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.6936 - g_loss: 1.3775\n",
            "Epoch 157/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5790 - g_loss: 1.0801\n",
            "Epoch 158/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5363 - g_loss: 1.3883\n",
            "Epoch 159/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5539 - g_loss: 1.0922\n",
            "Epoch 160/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5429 - g_loss: 1.4235\n",
            "Epoch 161/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.4749 - g_loss: 1.4654\n",
            "Epoch 162/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5880 - g_loss: 1.2737\n",
            "Epoch 163/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5342 - g_loss: 1.2858\n",
            "Epoch 164/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5620 - g_loss: 1.4965\n",
            "Epoch 165/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4958 - g_loss: 1.2599\n",
            "Epoch 166/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5811 - g_loss: 1.3713\n",
            "Epoch 167/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5489 - g_loss: 1.5061\n",
            "Epoch 168/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5721 - g_loss: 1.1733\n",
            "Epoch 169/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5669 - g_loss: 1.5793\n",
            "Epoch 170/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.4992 - g_loss: 1.2934\n",
            "Epoch 171/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5247 - g_loss: 1.2928\n",
            "Epoch 172/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5761 - g_loss: 1.6012\n",
            "Epoch 173/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5615 - g_loss: 1.3549\n",
            "Epoch 174/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5391 - g_loss: 1.2861\n",
            "Epoch 175/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.5516 - g_loss: 1.4591\n",
            "Epoch 176/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5309 - g_loss: 1.3299\n",
            "Epoch 177/800\n",
            "120/120 [==============================] - 81s 674ms/step - d_loss: 0.5431 - g_loss: 1.3548\n",
            "Epoch 178/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.5253 - g_loss: 1.3317\n",
            "Epoch 179/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.5738 - g_loss: 1.3499\n",
            "Epoch 180/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4774 - g_loss: 1.6865\n",
            "Epoch 181/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4475 - g_loss: 1.8713\n",
            "Epoch 182/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4936 - g_loss: 1.7145\n",
            "Epoch 183/800\n",
            "120/120 [==============================] - 81s 673ms/step - d_loss: 0.4959 - g_loss: 1.7274\n",
            "Epoch 184/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4896 - g_loss: 1.4656\n",
            "Epoch 185/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4305 - g_loss: 1.5370\n",
            "Epoch 186/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.4342 - g_loss: 1.5824\n",
            "Epoch 187/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.4852 - g_loss: 1.7584\n",
            "Epoch 188/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.5598 - g_loss: 1.5483\n",
            "Epoch 189/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.4592 - g_loss: 1.5123\n",
            "Epoch 190/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4354 - g_loss: 1.6188\n",
            "Epoch 191/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4679 - g_loss: 1.5892\n",
            "Epoch 192/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4839 - g_loss: 1.6308\n",
            "Epoch 193/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4373 - g_loss: 1.6805\n",
            "Epoch 194/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4406 - g_loss: 1.7936\n",
            "Epoch 195/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4862 - g_loss: 1.7117\n",
            "Epoch 196/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4155 - g_loss: 1.6301\n",
            "Epoch 197/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4019 - g_loss: 1.7379\n",
            "Epoch 198/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4652 - g_loss: 1.6741\n",
            "Epoch 199/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4778 - g_loss: 1.9923\n",
            "Epoch 200/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.5196 - g_loss: 1.5092\n",
            "Epoch 201/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4188 - g_loss: 1.5549\n",
            "Epoch 202/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4754 - g_loss: 1.5173\n",
            "Epoch 203/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3959 - g_loss: 1.8052\n",
            "Epoch 204/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4523 - g_loss: 1.9564\n",
            "Epoch 205/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3984 - g_loss: 1.6871\n",
            "Epoch 206/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.3977 - g_loss: 1.7131\n",
            "Epoch 207/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.3906 - g_loss: 1.8238\n",
            "Epoch 208/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.4221 - g_loss: 1.8819\n",
            "Epoch 209/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4177 - g_loss: 1.8311\n",
            "Epoch 210/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4898 - g_loss: 2.1735\n",
            "Epoch 211/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.4383 - g_loss: 1.7575\n",
            "Epoch 212/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3570 - g_loss: 1.8222\n",
            "Epoch 213/800\n",
            "120/120 [==============================] - 81s 678ms/step - d_loss: 0.4520 - g_loss: 1.8642\n",
            "Epoch 214/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.4503 - g_loss: 1.8425\n",
            "Epoch 215/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.4154 - g_loss: 1.7310\n",
            "Epoch 216/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.3932 - g_loss: 1.7309\n",
            "Epoch 217/800\n",
            "120/120 [==============================] - 81s 677ms/step - d_loss: 0.4272 - g_loss: 1.7874\n",
            "Epoch 218/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4065 - g_loss: 1.7847\n",
            "Epoch 219/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3908 - g_loss: 2.0636\n",
            "Epoch 220/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3821 - g_loss: 1.9647\n",
            "Epoch 221/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4416 - g_loss: 2.0487\n",
            "Epoch 222/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3920 - g_loss: 1.8030\n",
            "Epoch 223/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.4234 - g_loss: 1.7284\n",
            "Epoch 224/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.3881 - g_loss: 1.7237\n",
            "Epoch 225/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4054 - g_loss: 1.7979\n",
            "Epoch 226/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.4454 - g_loss: 2.3386\n",
            "Epoch 227/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3954 - g_loss: 1.8203\n",
            "Epoch 228/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3969 - g_loss: 1.7708\n",
            "Epoch 229/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3796 - g_loss: 1.8680\n",
            "Epoch 230/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.3892 - g_loss: 1.7775\n",
            "Epoch 231/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4122 - g_loss: 1.7697\n",
            "Epoch 232/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.4161 - g_loss: 1.8843\n",
            "Epoch 233/800\n",
            "120/120 [==============================] - 81s 676ms/step - d_loss: 0.3876 - g_loss: 1.8803\n",
            "Epoch 234/800\n",
            "120/120 [==============================] - 81s 675ms/step - d_loss: 0.3940 - g_loss: 2.0012\n",
            "Epoch 235/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.3912 - g_loss: 1.9885\n",
            "Epoch 236/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3901 - g_loss: 2.0072\n",
            "Epoch 237/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3689 - g_loss: 1.9234\n",
            "Epoch 238/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3953 - g_loss: 2.2611\n",
            "Epoch 239/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3822 - g_loss: 1.8252\n",
            "Epoch 240/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4036 - g_loss: 1.8343\n",
            "Epoch 241/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3890 - g_loss: 1.8853\n",
            "Epoch 242/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3818 - g_loss: 1.8367\n",
            "Epoch 243/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4265 - g_loss: 1.8021\n",
            "Epoch 244/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3511 - g_loss: 1.9263\n",
            "Epoch 245/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4008 - g_loss: 1.8033\n",
            "Epoch 246/800\n",
            "120/120 [==============================] - 81s 672ms/step - d_loss: 0.3861 - g_loss: 1.8993\n",
            "Epoch 247/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3465 - g_loss: 2.2608\n",
            "Epoch 248/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4054 - g_loss: 1.7552\n",
            "Epoch 249/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4188 - g_loss: 1.6919\n",
            "Epoch 250/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4073 - g_loss: 1.6791\n",
            "Epoch 251/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4244 - g_loss: 1.7024\n",
            "Epoch 252/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3987 - g_loss: 1.8073\n",
            "Epoch 253/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.4079 - g_loss: 1.9050\n",
            "Epoch 254/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3811 - g_loss: 1.8963\n",
            "Epoch 255/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3932 - g_loss: 2.0489\n",
            "Epoch 256/800\n",
            "120/120 [==============================] - 81s 671ms/step - d_loss: 0.3925 - g_loss: 1.9801\n",
            "Epoch 257/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3874 - g_loss: 1.7762\n",
            "Epoch 258/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3814 - g_loss: 1.6981\n",
            "Epoch 259/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.3972 - g_loss: 1.7263\n",
            "Epoch 260/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4140 - g_loss: 1.7484\n",
            "Epoch 261/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.4161 - g_loss: 1.8970\n",
            "Epoch 262/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.4151 - g_loss: 2.0108\n",
            "Epoch 263/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4065 - g_loss: 1.7756\n",
            "Epoch 264/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4138 - g_loss: 1.6250\n",
            "Epoch 265/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4113 - g_loss: 1.6559\n",
            "Epoch 266/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4027 - g_loss: 1.6859\n",
            "Epoch 267/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4275 - g_loss: 1.5556\n",
            "Epoch 268/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4431 - g_loss: 1.6693\n",
            "Epoch 269/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4279 - g_loss: 1.8723\n",
            "Epoch 270/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4237 - g_loss: 1.7308\n",
            "Epoch 271/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4257 - g_loss: 1.5376\n",
            "Epoch 272/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4277 - g_loss: 1.5928\n",
            "Epoch 273/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4357 - g_loss: 1.5758\n",
            "Epoch 274/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4261 - g_loss: 1.5562\n",
            "Epoch 275/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4243 - g_loss: 1.5697\n",
            "Epoch 276/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4553 - g_loss: 1.5339\n",
            "Epoch 277/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4356 - g_loss: 1.5754\n",
            "Epoch 278/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4530 - g_loss: 1.5424\n",
            "Epoch 279/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.4438 - g_loss: 1.4524\n",
            "Epoch 280/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4377 - g_loss: 1.5215\n",
            "Epoch 281/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4387 - g_loss: 1.4814\n",
            "Epoch 282/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.4274 - g_loss: 1.5153\n",
            "Epoch 283/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4212 - g_loss: 1.5569\n",
            "Epoch 284/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.4385 - g_loss: 1.4905\n",
            "Epoch 285/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4341 - g_loss: 1.4871\n",
            "Epoch 286/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4491 - g_loss: 1.5030\n",
            "Epoch 287/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4290 - g_loss: 1.5505\n",
            "Epoch 288/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4383 - g_loss: 1.4923\n",
            "Epoch 289/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4148 - g_loss: 1.5043\n",
            "Epoch 290/800\n",
            "120/120 [==============================] - 81s 670ms/step - d_loss: 0.4427 - g_loss: 1.5896\n",
            "Epoch 291/800\n",
            "120/120 [==============================] - 80s 669ms/step - d_loss: 0.4086 - g_loss: 1.8744\n",
            "Epoch 292/800\n",
            "120/120 [==============================] - 80s 670ms/step - d_loss: 0.3892 - g_loss: 1.8318\n",
            "Epoch 293/800\n",
            " 15/120 [==>...........................] - ETA: 1:10 - d_loss: 0.4345 - g_loss: 1.5754"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = p(np.random.randn(1,128))"
      ],
      "metadata": {
        "id": "2JjGKWB8qUQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.squeeze(img)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "AJlC2DIiqViO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.generator.save('./generator_naruto_long.hd5')"
      ],
      "metadata": {
        "id": "nKfapBYSAm6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r generator_naruto_long.hd5.zip generator_naruto_long.hd5"
      ],
      "metadata": {
        "id": "r-bJkC-vCdJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"generator_naruto_long.hd5.zip\")"
      ],
      "metadata": {
        "id": "epvaAkCvDPJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r naruto-faces1.zip naruto-faces1\n",
        "!zip -r naruto-faces2.zip naruto-faces2\n",
        "!zip -r naruto-faces3.zip naruto-faces3\n",
        "files.download(\"naruto-faces1.zip\")\n",
        "files.download(\"naruto-faces2.zip\")\n",
        "files.download(\"naruto-faces3.zip\")"
      ],
      "metadata": {
        "id": "ypsAw3nDbJXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}